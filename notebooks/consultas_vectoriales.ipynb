{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aabde6a",
   "metadata": {
    "id": "9aabde6a"
   },
   "source": [
    "\n",
    "# Consultas Vectoriales en LanceDB\n",
    "\n",
    "## Introducción\n",
    "Las consultas vectoriales permiten encontrar elementos similares basados en representaciones vectoriales de datos, como texto, imágenes o audio.  **LanceDB** es una base de datos optimizada para búsquedas vectoriales, ideal para casos de uso como recuperación de información, sistemas de recomendación y análisis multimodal.\n",
    "\n",
    "En este notebook aprenderemos a:\n",
    "1. Generar embeddings para distintos tipos de datos: texto, imágenes, audio y multimodal.\n",
    "2. Almacenar y consultar estos embeddings en LanceDB.\n",
    "3. Realizar búsquedas vectoriales y combinadas.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b996dd5f",
   "metadata": {
    "id": "b996dd5f"
   },
   "source": [
    "\n",
    "## 1. Configuración Inicial\n",
    "\n",
    "Primero, configuraremos el entorno para trabajar con LanceDB y las librerías necesarias para generar embeddings de texto, imágenes y audio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dce36d52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dce36d52",
    "outputId": "a29e14ee-f554-4701-fb57-33f7287ea7b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "import lancedb\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image  # Para manejo de imágenes\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision import transforms\n",
    "import librosa  \n",
    "import os  \n",
    "import pyarrow as pa\n",
    "# Confirmar que las bibliotecas se importaron correctamente \n",
    "print(\"Bibliotecas importadas correctamente.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f353e4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f353e4d",
    "outputId": "4783ca44-1083-46be-8812-9f07eea3d5c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de datos configurada en: data/vectorial_db\n"
     ]
    }
   ],
   "source": [
    "# Configurar LanceDB\n",
    "db_path = \"data/vectorial_db\"  # Ruta donde se almacenará la base de datos\n",
    "\n",
    "# Crear el directorio si no existe\n",
    "os.makedirs(db_path, exist_ok=True)\n",
    "\n",
    "# Conectar a LanceDB\n",
    "db = lancedb.connect(db_path)\n",
    "\n",
    "print(f\"Base de datos configurada en: {db_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ff3e8",
   "metadata": {
    "id": "fa3ff3e8"
   },
   "source": [
    "\n",
    "## 2. Generar Embeddings de Texto\n",
    "\n",
    "Utilizaremos **sentence-transformers** para convertir texto en representaciones vectoriales que puedan almacenarse y consultarse en LanceDB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04a20bff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "04a20bff",
    "outputId": "9b4561bd-6115-4a8e-c12c-331d3bb204e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla 'text_embeddings' creada con éxito con pa.list_.\n"
     ]
    }
   ],
   "source": [
    "# Cargar modelo para embeddings de texto\n",
    "text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Datos de ejemplo\n",
    "text_data = [\n",
    "    \"El cielo es azul y hermoso.\",\n",
    "    \"Los gatos son mascotas populares.\",\n",
    "    \"La inteligencia artificial está transformando el mundo.\",\n",
    "    \"Las flores son coloridas y fragantes.\",\n",
    "    \"Viajar es una experiencia enriquecedora.\"\n",
    "]\n",
    "\n",
    "# Generar embeddings\n",
    "text_embeddings = text_model.encode(text_data)\n",
    "text_embeddings = [embedding.astype(np.float32).tolist() for embedding in text_embeddings]  # Convertir a float32\n",
    "\n",
    "# Crear DataFrame con los datos y embeddings\n",
    "df_text = pd.DataFrame({\n",
    "    \"texto\": text_data,\n",
    "    \"embedding\": text_embeddings\n",
    "})\n",
    "\n",
    "# Convertir a una tabla Arrow con el esquema simulado usando pa.list_\n",
    "schema = pa.schema([\n",
    "    (\"texto\", pa.string()),  # Columna de texto\n",
    "    (\"embedding\", pa.list_(pa.float32(), list_size=len(text_embeddings[0])))  # Lista con tamaño esperado\n",
    "])\n",
    "\n",
    "# Crear la tabla Arrow\n",
    "table = pa.Table.from_pandas(df_text, schema=schema)\n",
    "\n",
    "# Guardar la tabla en LanceDB\n",
    "tabla_text = db.create_table(\"text_embeddings\", data=table, mode=\"overwrite\")\n",
    "print(\"Tabla 'text_embeddings' creada con éxito con pa.list_.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e7ef6a",
   "metadata": {
    "id": "99e7ef6a"
   },
   "source": [
    "\n",
    "## 3. Generar Embeddings de Imágenes\n",
    "\n",
    "Usaremos un modelo preentrenado como CLIP para convertir imágenes en vectores representativos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b0e0fa8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b0e0fa8",
    "outputId": "389a0781-1901-483c-a133-61199da7c085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generados para imágenes:\n",
      "         imagen                                          embedding\n",
      "0  imagen_1.jpg  [4.202572822570801, -2.4697437286376953, 1.471...\n",
      "1  imagen_6.jpg  [2.447737216949463, 1.3279236555099487, -0.803...\n",
      "2  imagen_5.jpg  [3.017625570297241, 1.0321232080459595, 0.4439...\n",
      "3  imagen_4.jpg  [5.526756286621094, 7.365933895111084, 1.59381...\n",
      "4  imagen_3.jpg  [2.2661871910095215, -1.88070547580719, 0.1356...\n",
      "Tabla 'image_embeddings' creada con éxito.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-01T02:54:38Z WARN  lance::dataset] No existing dataset at /home/jake3120/itam/FuentesDeDatos/LanceDB/notebooks/data/vectorial_db/image_embeddings.lance, it will be created\n"
     ]
    }
   ],
   "source": [
    "# Configuración del modelo y transformaciones\n",
    "image_model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)  # Usar el parámetro 'weights' para evitar advertencias\n",
    "image_model.eval()\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Ruta a la carpeta de imágenes\n",
    "image_dir = \"data/images\"\n",
    "\n",
    "# Procesar cada imagen\n",
    "image_embeddings = []\n",
    "image_data = []\n",
    "\n",
    "for image_file in os.listdir(image_dir):\n",
    "    if image_file.endswith(\".jpg\") or image_file.endswith(\".png\"):\n",
    "        img_path = os.path.join(image_dir, image_file)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = image_transform(img).unsqueeze(0)\n",
    "\n",
    "        # Generar embedding\n",
    "        with torch.no_grad():\n",
    "            embedding = image_model(img_tensor).squeeze().numpy().astype('float32')\n",
    "\n",
    "        image_data.append(image_file)  # Nombre de la imagen\n",
    "        image_embeddings.append(embedding.tolist())  # Embedding como lista\n",
    "\n",
    "# Crear DataFrame con los datos y embeddings\n",
    "df_images = pd.DataFrame({\n",
    "    \"imagen\": image_data,  # Nombre de la imagen\n",
    "    \"embedding\": image_embeddings  # Embedding de la imagen\n",
    "})\n",
    "\n",
    "print(\"Embeddings generados para imágenes:\")\n",
    "print(df_images.head())\n",
    "\n",
    "# Guardar en LanceDB\n",
    "# Crear esquema usando pa.list_\n",
    "schema = pa.schema([\n",
    "    (\"imagen\", pa.string()),  # Nombre de la imagen\n",
    "    (\"embedding\", pa.list_(pa.float32(), list_size=len(image_embeddings[0])))  # Lista de tamaño esperado\n",
    "])\n",
    "\n",
    "# Crear tabla Arrow\n",
    "table = pa.Table.from_pandas(df_images, schema=schema)\n",
    "\n",
    "# Guardar la tabla en LanceDB\n",
    "tabla_images = db.create_table(\"image_embeddings\", data=table, mode=\"overwrite\")\n",
    "print(\"Tabla 'image_embeddings' creada con éxito.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8708ce51",
   "metadata": {
    "id": "8708ce51"
   },
   "source": [
    "## 4. Generar Embeddings de Audio\n",
    "\n",
    "Usaremos **librosa** para extraer características de audio y convertirlas en vectores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bd735ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bd735ff",
    "outputId": "ddbbea3f-13be-477a-c5e5-b06be009b6b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generados para audios:\n",
      "          audio                                          embedding\n",
      "0  esponja4.mp3  [-185.07440185546875, 130.5316619873047, -56.4...\n",
      "1  esponja2.mp3  [-175.2745819091797, 88.03491973876953, -27.00...\n",
      "2  esponja1.mp3  [-103.59510040283203, 124.67962646484375, -31....\n",
      "3  esponja3.mp3  [-105.74647521972656, 64.4498062133789, -136.2...\n",
      "Tabla 'audio_embeddings' creada con éxito.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-01T02:55:05Z WARN  lance::dataset] No existing dataset at /home/jake3120/itam/FuentesDeDatos/LanceDB/notebooks/data/vectorial_db/audio_embeddings.lance, it will be created\n"
     ]
    }
   ],
   "source": [
    "# Ruta a la carpeta de audios\n",
    "audio_dir = \"data/audios\"  \n",
    "\n",
    "# Procesar cada archivo de audio\n",
    "audio_data = []\n",
    "audio_embeddings = []\n",
    "\n",
    "# Configuración de MFCC\n",
    "n_mfcc = 13  # Tamaño fijo para los embeddings\n",
    "embedding_length = n_mfcc  # Aseguramos un tamaño fijo\n",
    "\n",
    "for audio_file in os.listdir(audio_dir):\n",
    "    if audio_file.endswith(\".wav\") or audio_file.endswith(\".mp3\"):\n",
    "        audio_path = os.path.join(audio_dir, audio_file)\n",
    "        \n",
    "        # Cargar el audio\n",
    "        y, sr = librosa.load(audio_path, sr=None)  # sr=None conserva la tasa original\n",
    "\n",
    "        # Extraer MFCC\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        \n",
    "        # Generar embedding de tamaño fijo (promediando)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1).astype(np.float32)  # Tamaño fijo de n_mfcc\n",
    "\n",
    "        # Guardar los datos\n",
    "        audio_data.append(audio_file)  # Nombre del archivo\n",
    "        audio_embeddings.append(mfcc_mean.tolist())  # Embedding como lista\n",
    "\n",
    "# Crear DataFrame con los datos y embeddings\n",
    "df_audio = pd.DataFrame({\n",
    "    \"audio\": audio_data,\n",
    "    \"embedding\": audio_embeddings\n",
    "})\n",
    "\n",
    "print(\"Embeddings generados para audios:\")\n",
    "print(df_audio.head())\n",
    "\n",
    "# Crear esquema usando pa.list_ para guardar en LanceDB\n",
    "schema = pa.schema([\n",
    "    (\"audio\", pa.string()),  # Nombre del archivo de audio\n",
    "    (\"embedding\", pa.list_(pa.float32(), list_size=embedding_length))  # Embedding de tamaño fijo\n",
    "])\n",
    "\n",
    "# Convertir DataFrame a tabla Arrow\n",
    "table = pa.Table.from_pandas(df_audio, schema=schema)\n",
    "\n",
    "# Guardar en LanceDB\n",
    "tabla_audio = db.create_table(\"audio_embeddings\", data=table, mode=\"overwrite\")\n",
    "print(\"Tabla 'audio_embeddings' creada con éxito.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f15fdd",
   "metadata": {
    "id": "55f15fdd"
   },
   "source": [
    "# Consultas Vectoriales\n",
    "\n",
    "Realizaremos búsquedas vectoriales en LanceDB para encontrar elementos similares.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b3d97",
   "metadata": {},
   "source": [
    "## Consultas sobre los embeddings de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16660199",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "16660199",
    "outputId": "4145777c-3036-4115-b5d9-290c2dc1311d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de búsqueda para texto (consulta 1):\n",
      "                                      texto  \\\n",
      "0     Las flores son coloridas y fragantes.   \n",
      "1  Viajar es una experiencia enriquecedora.   \n",
      "2               El cielo es azul y hermoso.   \n",
      "\n",
      "                                           embedding  _distance  \n",
      "0  [-0.016914625, -0.012992999, -0.02974755, 0.02...   0.451207  \n",
      "1  [0.06420154, 0.026209284, -0.03604845, 0.05888...   1.035577  \n",
      "2  [-0.003502195, 0.08340518, -0.04015484, -0.000...   1.148967  \n"
     ]
    }
   ],
   "source": [
    "# Consulta 1: Tema general\n",
    "query_text_1 = \"Las flores tienen colores brillantes.\"  # Texto de consulta\n",
    "query_embedding_1 = text_model.encode([query_text_1])[0].astype(np.float32)  # Generar embedding\n",
    "\n",
    "# Buscar en la tabla\n",
    "results_text_1 = tabla_text.search(query_embedding_1, vector_column_name=\"embedding\").limit(3).to_pandas()\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Resultados de búsqueda para texto (consulta 1):\")\n",
    "print(results_text_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adaef6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de búsqueda para texto (consulta 2):\n",
      "                                               texto  \\\n",
      "0  La inteligencia artificial está transformando ...   \n",
      "1                        El cielo es azul y hermoso.   \n",
      "2           Viajar es una experiencia enriquecedora.   \n",
      "\n",
      "                                           embedding  _distance  \n",
      "0  [-0.049482808, 0.057707522, -0.0005006774, -0....   0.726331  \n",
      "1  [-0.003502195, 0.08340518, -0.04015484, -0.000...   1.063330  \n",
      "2  [0.06420154, 0.026209284, -0.03604845, 0.05888...   1.256500  \n"
     ]
    }
   ],
   "source": [
    "# Consulta 2: Tema específico\n",
    "query_text_2 = \"La inteligencia artificial cambiará el futuro.\"  # Texto de consulta\n",
    "query_embedding_2 = text_model.encode([query_text_2])[0].astype(np.float32)  # Generar embedding\n",
    "\n",
    "# Buscar en la tabla\n",
    "results_text_2 = tabla_text.search(query_embedding_2, vector_column_name=\"embedding\").limit(3).to_pandas()\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Resultados de búsqueda para texto (consulta 2):\")\n",
    "print(results_text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf2510",
   "metadata": {},
   "source": [
    "## Consultas sobre los embeddings de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8239c9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "e8239c9b",
    "outputId": "00b6b128-4520-419a-f07b-3fcb3c1f4844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de búsqueda para la consulta 1:\n",
      "         imagen                                          embedding  \\\n",
      "0  imagen_5.jpg  [3.0176253, 1.0321219, 0.4439306, -0.8215103, ...   \n",
      "1  imagen_3.jpg  [2.2661877, -1.8807054, 0.13566248, -1.3516074...   \n",
      "2  imagen_1.jpg  [4.202572, -2.469743, 1.4718562, 0.99616164, 2...   \n",
      "\n",
      "     _distance  \n",
      "0  3776.850830  \n",
      "1  4464.903809  \n",
      "2  6996.564941  \n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Crear una imagen de consulta (colores suaves)\n",
    "query_image_1 = Image.new(\"RGB\", (224, 224), (80, 120, 160))  # Color RGB suave\n",
    "query_image_tensor_1 = image_transform(query_image_1).unsqueeze(0)\n",
    "\n",
    "# Generar embedding para la imagen de consulta\n",
    "with torch.no_grad():\n",
    "    query_image_embedding_1 = image_model(query_image_tensor_1).squeeze().numpy().astype('float32')\n",
    "\n",
    "# Realizar la búsqueda en LanceDB\n",
    "results_image_1 = tabla_images.search(query_image_embedding_1, vector_column_name=\"embedding\").limit(3).to_pandas()\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Resultados de búsqueda para la consulta 1:\")\n",
    "print(results_image_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c80cb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de búsqueda para la consulta 2:\n",
      "         imagen                                          embedding  \\\n",
      "0  imagen_6.jpg  [2.4477372, 1.3279237, -0.8030599, -3.3387642,...   \n",
      "1  imagen_5.jpg  [3.0176256, 1.0321232, 0.44393137, -0.8215101,...   \n",
      "2  imagen_1.jpg  [4.202573, -2.4697437, 1.4718571, 0.9961633, 2...   \n",
      "\n",
      "     _distance  \n",
      "0     0.000000  \n",
      "1  5792.356934  \n",
      "2  6944.045410  \n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Cargar una imagen de ejemplo desde el disco\n",
    "query_image_path_2 = \"data/images/imagen_6.jpg\" \n",
    "query_image_2 = Image.open(query_image_path_2).convert(\"RGB\")\n",
    "query_image_tensor_2 = image_transform(query_image_2).unsqueeze(0)\n",
    "\n",
    "# Generar embedding para la imagen de consulta\n",
    "with torch.no_grad():\n",
    "    query_image_embedding_2 = image_model(query_image_tensor_2).squeeze().numpy().astype('float32')\n",
    "\n",
    "# Realizar la búsqueda en LanceDB\n",
    "results_image_2 = tabla_images.search(query_image_embedding_2, vector_column_name=\"embedding\").limit(3).to_pandas()\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Resultados de búsqueda para la consulta 2:\")\n",
    "print(results_image_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0d79743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de búsqueda para la consulta 1:\n",
      "          audio                                          embedding  \\\n",
      "0  esponja4.mp3  [-185.0744, 130.53166, -56.47632, 24.994247, -...   \n",
      "1  esponja2.mp3  [-175.27458, 88.03492, -27.005796, 20.219791, ...   \n",
      "2  esponja1.mp3  [-103.5951, 124.67963, -31.355947, 32.714035, ...   \n",
      "\n",
      "     _distance  \n",
      "0     0.000000  \n",
      "1  4658.028809  \n",
      "2  8948.547852  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Seleccionar un audio de la tabla para la consulta\n",
    "query_audio_path = \"data/audios/esponja4.mp3\"  \n",
    "y, sr = librosa.load(query_audio_path, sr=None)\n",
    "\n",
    "# Extraer características (MFCC) y generar embedding\n",
    "query_mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "query_audio_embedding = np.mean(query_mfcc, axis=1).astype(np.float32)\n",
    "\n",
    "# Realizar la búsqueda en LanceDB\n",
    "results_audio = tabla_audio.search(query_audio_embedding, vector_column_name=\"embedding\").limit(3).to_pandas()\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Resultados de búsqueda para la consulta 1:\")\n",
    "print(results_audio)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe2591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de búsqueda para la consulta 1:\n",
      "          audio                                          embedding  \\\n",
      "0  esponja1.mp3  [-103.5951, 124.67963, -31.355947, 32.714035, ...   \n",
      "1  esponja2.mp3  [-175.27458, 88.03492, -27.005796, 20.219791, ...   \n",
      "2  esponja4.mp3  [-185.0744, 130.53166, -56.47632, 24.994247, -...   \n",
      "\n",
      "     _distance  \n",
      "0     0.000000  \n",
      "1  7900.018555  \n",
      "2  8948.547852  \n"
     ]
    }
   ],
   "source": [
    "# Seleccionar un audio de la tabla para la consulta\n",
    "query_audio_path = \"data/audios/esponja1.mp3\"  \n",
    "y, sr = librosa.load(query_audio_path, sr=None)\n",
    "\n",
    "# Extraer características (MFCC) y generar embedding\n",
    "query_mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "query_audio_embedding = np.mean(query_mfcc, axis=1).astype(np.float32)\n",
    "\n",
    "# Realizar la búsqueda en LanceDB\n",
    "results_audio = tabla_audio.search(query_audio_embedding, vector_column_name=\"embedding\").limit(3).to_pandas()\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Resultados de búsqueda para la consulta 1:\")\n",
    "print(results_audio)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
